{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6349afb6",
   "metadata": {},
   "source": [
    "# Hacker News DAG Pipeline\n",
    "### An Example Project Illustrating a Multiple Dependency Pipeline with Directed Acyclic Graph Implementation\n",
    "\n",
    "Our goal is to identify the most frequent keywords used in titles of popular stories on [Hacker News](https://news.ycombinator.com/) during 2014.  Starting with a json file of 2014 story data from a Hacker News API, we will implement a pipeline utilizing a DAG to extract the top 100 most frequent title words.\n",
    "\n",
    "Pipeline Overview:\n",
    "- Instantiate Pipeline object\n",
    "- Assign tasks:\n",
    "    - Load data from json file\n",
    "    - Filter for most popular stories\n",
    "    - Convert to csv format\n",
    "    - Extract titles\n",
    "    - Clean titles\n",
    "    - Count word frequency\n",
    "    - Find top 100 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd397bef",
   "metadata": {},
   "source": [
    "### Load Libraries and Dependencies\n",
    "- `Pipeline` and `build_csv` implementations are imported from [pipeline.py](https://github.com/teresahanak/hacker_news_dag_pipeline/blob/main/pipeline.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b10015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nfrom pipeline import Pipeline\\nimport json\\nimport csv\\nfrom pipeline import build_csv\\nimport io\\nimport datetime\\nimport string\\nimport stop_words\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nfrom pipeline import Pipeline\\nimport json\\nimport csv\\nfrom pipeline import build_csv\\nimport io\\nimport datetime\\nimport string\\nimport stop_words\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "from pipeline import Pipeline\n",
    "import json\n",
    "import csv\n",
    "from pipeline import build_csv\n",
    "import io\n",
    "import datetime\n",
    "import string\n",
    "import stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc388e81",
   "metadata": {},
   "source": [
    "### Utilizing the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6b82f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hn', 210),\n",
       " ('show', 192),\n",
       " ('new', 185),\n",
       " ('google', 167),\n",
       " ('bitcoin', 101),\n",
       " ('open', 92),\n",
       " ('programming', 90),\n",
       " ('web', 89),\n",
       " ('data', 85),\n",
       " ('us', 85),\n",
       " ('video', 79),\n",
       " ('python', 76),\n",
       " ('code', 72),\n",
       " ('using', 71),\n",
       " ('facebook', 71),\n",
       " ('released', 71),\n",
       " ('now', 69),\n",
       " ('2013', 65),\n",
       " ('javascript', 65),\n",
       " ('free', 64),\n",
       " ('source', 64),\n",
       " ('internet', 63),\n",
       " ('game', 63),\n",
       " ('first', 62),\n",
       " ('go', 60),\n",
       " ('will', 59),\n",
       " ('microsoft', 59),\n",
       " ('one', 59),\n",
       " ('c', 59),\n",
       " ('linux', 58),\n",
       " ('app', 57),\n",
       " ('can', 56),\n",
       " ('pdf', 55),\n",
       " ('work', 54),\n",
       " ('language', 54),\n",
       " ('dont', 54),\n",
       " ('software', 52),\n",
       " ('2014', 52),\n",
       " ('startup', 51),\n",
       " ('apple', 50),\n",
       " ('use', 50),\n",
       " ('make', 50),\n",
       " ('time', 48),\n",
       " ('yc', 48),\n",
       " ('security', 48),\n",
       " ('get', 45),\n",
       " ('nsa', 45),\n",
       " ('github', 45),\n",
       " ('system', 44),\n",
       " ('windows', 44),\n",
       " ('1', 41),\n",
       " ('world', 41),\n",
       " ('way', 41),\n",
       " ('like', 41),\n",
       " ('project', 40),\n",
       " ('computer', 40),\n",
       " ('heartbleed', 40),\n",
       " ('git', 37),\n",
       " ('users', 37),\n",
       " ('design', 37),\n",
       " ('ios', 37),\n",
       " ('back', 36),\n",
       " ('developer', 36),\n",
       " ('os', 36),\n",
       " ('twitter', 36),\n",
       " ('ceo', 36),\n",
       " ('vs', 36),\n",
       " ('life', 36),\n",
       " ('big', 35),\n",
       " ('day', 35),\n",
       " ('android', 34),\n",
       " ('online', 34),\n",
       " ('made', 33),\n",
       " ('years', 33),\n",
       " ('simple', 33),\n",
       " ('court', 33),\n",
       " ('guide', 32),\n",
       " ('learning', 32),\n",
       " ('mt', 32),\n",
       " ('api', 32),\n",
       " ('says', 32),\n",
       " ('apps', 32),\n",
       " ('browser', 32),\n",
       " ('server', 31),\n",
       " ('firefox', 31),\n",
       " ('fast', 31),\n",
       " ('amazon', 31),\n",
       " ('gox', 31),\n",
       " ('problem', 31),\n",
       " ('mozilla', 31),\n",
       " ('engine', 31),\n",
       " ('site', 31),\n",
       " ('introducing', 30),\n",
       " ('year', 30),\n",
       " ('support', 29),\n",
       " ('stop', 29),\n",
       " ('built', 29),\n",
       " ('better', 29),\n",
       " ('million', 29),\n",
       " ('people', 29)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"results = pipeline.run()\\nresults[top_100_title_words]\";\n",
       "                var nbb_formatted_code = \"results = pipeline.run()\\nresults[top_100_title_words]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiating pipeline object\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Loading stories in json file as list of dictionaries for each story\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open(\"hn_stories_2014.json\") as f:\n",
    "        stories = json.load(f)[\"stories\"]\n",
    "    return stories\n",
    "\n",
    "# Filtering into generator of most popular stories\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    return (\n",
    "        story\n",
    "        for story in stories\n",
    "        if (story[\"points\"] > 50)\n",
    "        and (story[\"num_comments\"] > 1)\n",
    "        and not (story[\"title\"].startswith(\"Ask HN\"))\n",
    "    )\n",
    "\n",
    "# Converting to csv file format as io.StringIO object for ease of use\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = [\n",
    "        [\n",
    "            story[\"objectID\"],\n",
    "            datetime.datetime.strptime(\n",
    "                story[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "            ),  # Sample format: 2014-05-29T04:27:42Z\n",
    "            story[\"url\"],\n",
    "            story[\"points\"],\n",
    "            story[\"title\"],\n",
    "        ]\n",
    "        for story in stories\n",
    "    ]\n",
    "    return build_csv(\n",
    "        lines,\n",
    "        header=[\"objectID\", \"created_at\", \"url\", \"points\", \"title\"],\n",
    "        file=io.StringIO(),\n",
    "    )\n",
    "\n",
    "# Extracting titles into generator\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index(\"title\")\n",
    "    return (story[idx] for story in reader)\n",
    "\n",
    "# Cleaning titles into generator\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(titles):\n",
    "    return (\n",
    "        title.translate(title.maketrans(\"\", \"\", string.punctuation + \"‘’–\")).lower()\n",
    "        for title in titles\n",
    "    )\n",
    "\n",
    "# Counting frequency of words in titles into sorted list of (word, count) tuples, exluding stop_words\n",
    "@pipeline.task(depends_on=clean_titles)\n",
    "def word_freq_tbl_sorted(titles):\n",
    "    counts = {}\n",
    "    for title in titles:\n",
    "        for word in title.split():\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "\n",
    "    exclude = stop_words.get_stop_words(\"en\")\n",
    "    counts = {k: v for k, v in counts.items() if k not in exclude}\n",
    "    return sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Reducing list of words to top 100 most frequent\n",
    "@pipeline.task(depends_on=word_freq_tbl_sorted)\n",
    "def top_100_title_words(word_counts):\n",
    "    return word_counts[:100]\n",
    "\n",
    "results = pipeline.run()\n",
    "results[top_100_title_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a38f374",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- We were able to retrieve the information we sought very quickly and with minimal code.\n",
    "- Additionally, by relying on generators, RAM usage is contained.\n",
    "- We can see that there are still some less useful words in our results, so we could do another iteration adding to the `excluded` stop words for a more refined list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c62dd58",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This base execution of the DAG pipeline with multiple dependencies illustrates the efficiency of the approach both for data preparation and filtering for insights.  With the Pipeline and DAG classes established, they can be easily applied to any number of tasks on various datasets, as we have done here.  In a specific use case, such as the above example, the code is very readable and accessible for modification and adaptation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
